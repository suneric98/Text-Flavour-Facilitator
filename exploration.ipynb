{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "empty = \"<empty>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    emojiVocab.add(empty)\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)\n",
    "vocabIdx = {word : i for i, word in enumerate(vocab)}\n",
    "eVocabIdx = {emoji : i for i, emoji in enumerate(emojiVocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=19720, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "# Takes: dataset, word2vec model, and vocabulary from training\n",
    "# returns: list of tuples. First value is a torch of word embeddings for that sentence,\n",
    "# second value is the labels for each word\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '“', 'damnn', 'gurl', 'you', 'fine', 'a', 'fuck', '💦', '💦', '💦', 'did', 'you', 'fall', 'from', 'heaven', 'because', 'you', 'have', 'the', 'phattest', 'as', 'on', 'god', '🍑', '🍑', 'you', 'do', 'track', 'cool', 'because', 'i', 'can', 'track', 'dat', 'as', 'a', 'u', 'and', 'me', 'go', 'crazy', 'babey', '😎', 'hey', 'you', 'forgot', 'to', 'fill', 'out', 'this', 'survey', 'haha', 'just', 'kidding', 'that', '’', 's', 'my', 'phone', '😎', 'gim', 'me', 'ur', 'number', 'so', 'we', 'can', 'talk', 'all', 'night', '😉', 'what', '’', 's', 'that', 'you', '’', 're', 'single', 'haha', 'i', 'never', 'knew', 'but', 'what', 'a', 'coincidence', 'let', '’', 's', 'get', 'not', 'single', 'together', '😎', 'i', 'may', 'not', 'run', 'but', 'i', '’', 'm', 'boutta', 'run', 'up', 'on', 'dat', 'azz', '😤', 'haha', 'what', '’', 's', 'that', 'you', 'aren', '’', 't', 'looking', 'for', 'a', 'boyfriend', 'well', 'that', '’', 's', 'perfect', 'because', 'i', '’', 'm', 'no', 'boy', '😎', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "tensor([[-9.3177e-01, -2.9776e-01, -1.9192e+00,  ..., -4.1233e-01,\n",
      "          6.6682e-02,  1.1935e-01],\n",
      "        [-9.5044e-01, -2.1080e-01, -1.7289e+00,  ..., -4.2050e-01,\n",
      "          1.6278e-01, -7.8966e-02],\n",
      "        [-3.5400e-05,  1.2845e-02, -1.3731e-02,  ..., -1.0807e-02,\n",
      "          3.3908e-03, -2.3939e-03],\n",
      "        ...,\n",
      "        [-4.2613e-01, -9.1805e-02, -1.3128e+00,  ..., -1.4143e-01,\n",
      "          5.7582e-02,  1.6041e-01],\n",
      "        [-2.4548e+00, -1.4557e-02, -2.1166e+00,  ...,  4.2745e-01,\n",
      "         -1.1323e-01, -6.1282e-01],\n",
      "        [-1.3888e+00, -5.9507e-01, -1.8647e+00,  ...,  3.6282e-02,\n",
      "          7.8396e-01,  2.2058e-01]])\n",
      "['<s>', 'so', '😂', 'i', 'walked', '🚶\\u200d♂️', 'into', 'my', 'dad', 'bedroom', '🛌', 'and', 'i', 'saw', 'him', 'holding', 'a', 'real', 'lightsaber', '🍆', 'and', 'he', 'wa', 'stabbing', 'mommy', 'in', 'the', 'bum', 'bum', '😭', 'he', 'told', 'me', 'to', 'cum', 'here', 'so', 'i', 'did', 'i', 'wa', 'very', 'excited', '😛', 'to', 'hold', 'a', 'lightsaber', 'he', 'gave', 'it', 'to', 'me', 'and', 'it', 'vibrated', '😟', 'then', 'my', 'pee', 'pee', '🍆', '💦', 'started', 'to', 'grow', 'i', 'wa', 'so', 'scared', 'because', 'i', 'wa', 'a', 'girl', '👱\\u200d♀️', 'he', 'then', 'locked', 'the', 'door', 'and', 'he', 'stabbed', 'me', 'with', 'the', 'lightsaber', '</s>']\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([[-9.3177e-01, -2.9776e-01, -1.9192e+00,  ..., -4.1233e-01,\n",
      "          6.6682e-02,  1.1935e-01],\n",
      "        [-1.4069e+00,  3.9938e-01, -2.9254e+00,  ..., -7.8487e-01,\n",
      "         -2.9841e-01,  8.9971e-02],\n",
      "        [-2.4760e+00, -7.0373e-01, -2.3739e+00,  ..., -1.4086e+00,\n",
      "          1.0933e+00, -2.1641e-01],\n",
      "        ...,\n",
      "        [-6.3424e-02, -4.0094e-01, -2.6267e+00,  ..., -1.3940e-02,\n",
      "         -7.6613e-01,  9.9822e-01],\n",
      "        [-1.7779e-02,  3.9336e-04, -3.8356e-02,  ..., -5.8678e-03,\n",
      "          1.5905e-02,  2.8388e-03],\n",
      "        [-1.3888e+00, -5.9507e-01, -1.8647e+00,  ...,  3.6282e-02,\n",
      "          7.8396e-01,  2.2058e-01]])\n",
      "['<s>', 'you', 'can', 'call', 'me', 'by', 'my', 'middle', 'name', '🥰', 'my', 'middle', 'name', 'is', '💕', 'vanessa', '💕', 'you', 'can', 'call', 'me', 'by', 'my', 'middle', 'name', 'if', 'you', 'want', 'too', '😋', 'or', 'you', 'can', 'call', 'me', 'shawdy', 'or', 'shorty', '😻', 'or', 'whatever', 'you', 'want', 'ok', '👌🏽', '🤤', '😯', 'thts', 'the', 'fuckin', 'gucci', 'gang', 'shyte', '💢', '❗', '️and', 'that', '’', 's', 'the', 'fuckin', 'period', 'shit', '⚡', '️', '😎', 'yk', 'the', 'fuckin', 'vibe', '😏', 'ahahaaa', 'ahaaaaa', '😛', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "tensor([[-9.3177e-01, -2.9776e-01, -1.9192e+00,  ..., -4.1233e-01,\n",
      "          6.6682e-02,  1.1935e-01],\n",
      "        [-1.0108e+00,  1.0922e+00, -3.0663e+00,  ..., -1.0381e+00,\n",
      "         -3.2698e-01,  4.8048e-01],\n",
      "        [-1.2786e+00,  7.5444e-01, -2.8062e+00,  ..., -9.7397e-01,\n",
      "         -3.2740e-02, -2.4887e-01],\n",
      "        ...,\n",
      "        [-1.1541e-02, -1.0776e-02, -3.2272e-02,  ...,  1.1258e-03,\n",
      "          1.0345e-02,  3.9498e-03],\n",
      "        [-8.2448e-01,  6.8891e-02, -1.0926e+00,  ..., -2.2499e-01,\n",
      "          4.2927e-01,  7.9648e-02],\n",
      "        [-1.3888e+00, -5.9507e-01, -1.8647e+00,  ...,  3.6282e-02,\n",
      "          7.8396e-01,  2.2058e-01]])\n",
      "['<s>', 'seriously', 'fuck', 'plastic', 'bag', 'everybody', 'is', 'like', 'oh', 'i', 'love', 'you', 'plastic', 'bag', 'and', 'thx', 'for', 'catching', 'my', 'cummies', '💦', '💦', 'plastic', 'bag', 'but', 'fuck', 'that', 'i', 'say', 'revolt', '✊', 'against', 'the', 'plastic', 'bag', 'oligarchy', 'theyve', 'kept', 'u', 'down', 'for', 'too', 'long', 'our', 'grocery', 'our', 'sex', 'toy', 'and', 'our', 'freedom', 'ha', 'been', 'coopted', 'by', 'the', 'plastic', 'bag', 'mafia', 'they', 'are', 'building', 'a', 'new', 'continent', 'in', 'the', 'pacific', 'while', 'you', 'carry', 'home', 'a', 'loaf', 'of', 'bread', 'in', 'one', 'of', 'their', 'bag', 'of', 'oppression', 'no', 'more', '😡', 'join', 'u', 'at', 'wwwfabricbagsforfreedom', 'and', 'get', 'a', '50', 'discount', 'on', 'totally', 'american', '🇺🇸', 'stuff', 'that', 'will', 'make', 'your', 'willy', 'bigger', 'and', 'alienate', 'friend', 'and', 'family', '☝', '️', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "tensor([[-0.9318, -0.2978, -1.9192,  ..., -0.4123,  0.0667,  0.1194],\n",
      "        [-0.0707, -0.0125, -0.1605,  ..., -0.0228,  0.0104,  0.0171],\n",
      "        [-0.5972, -0.0288, -2.2873,  ..., -0.3659,  0.1251,  0.5230],\n",
      "        ...,\n",
      "        [-1.2698, -0.1526, -1.5284,  ...,  0.5004,  0.1613, -0.0963],\n",
      "        [-2.1716, -0.9804, -2.3757,  ...,  0.8917,  0.7259, -0.4686],\n",
      "        [-1.3888, -0.5951, -1.8647,  ...,  0.0363,  0.7840,  0.2206]])\n",
      "['<s>', '🔫', 'sorry', 'that', 'i', 'believe', 'in', 'the', 'first', 'amendment', '🔫', '🇱🇷', 'sorry', 'for', 'trying', 'to', 'keep', 'our', 'traditional', 'american', 'value', '🇱🇷', '🤬', 'sorry', 'for', 'obliterating', 'you', 'in', 'every', 'debate', '🤬', '🏳️\\u200d🌈', 'sorry', 'for', 'trying', 'to', 'keep', 'men', 'men', 'and', 'woman', 'woman', '🏳️\\u200d🌈', '🤔', 'sorry', 'for', 'being', 'the', 'one', 'with', 'common', 'sense', '🤔', 'so', 'in', 'conclusion', '💣', '⚔', '️don', '’', 't', 'fuck', 'with', 'u', '⚔', '️', '💣', '😈', 'because', 'we', 'will', 'use', 'our', 'fact', 'over', 'feeling', '😈', '👱🏼\\u200d♂️', 'and', 'put', 'these', 'beta', 'cucks', 'in', 'their', 'place', '👱🏼\\u200d♂️', '✊🏻', '✊🏼', 'in', 'shapiro', 'we', 'trust', '✊🏻', '✊🏼', '</s>']\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "tensor([[-0.9318, -0.2978, -1.9192,  ..., -0.4123,  0.0667,  0.1194],\n",
      "        [-0.7111, -0.2542, -1.7698,  ..., -0.0212,  0.1816,  0.1709],\n",
      "        [-0.5301,  0.0271, -1.4213,  ..., -0.2850, -0.0570,  0.1801],\n",
      "        ...,\n",
      "        [-0.1275, -0.0270, -0.3328,  ..., -0.0441,  0.0767,  0.0323],\n",
      "        [-0.0990, -0.0902, -0.3055,  ..., -0.0307,  0.0415,  0.0426],\n",
      "        [-1.3888, -0.5951, -1.8647,  ...,  0.0363,  0.7840,  0.2206]])\n"
     ]
    }
   ],
   "source": [
    "sample = train.sample(5)\n",
    "for index,row in sample.iterrows():\n",
    "    print(row[\"words\"])\n",
    "    print(row[\"labels\"])\n",
    "    print(trainEmb[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building supervised models to predict next emoji\n",
    "\n",
    "+ RNN based architecture where we look at the hidden layer for every word\n",
    "  + Using hidden layer, predict if is an emoji and what emoji it is\n",
    "+ Asked TA from NLP class, they said this is similar to a language modelling problem where we only predict the set of emoji vocabulary\n",
    "  + Could also view as sequence labelling where tag is next emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "jupyter = True\n",
    "if jupyter:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward NN\n",
    "\n",
    "+ https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "+ Built using previous couple of words\n",
    "+ Feature is word embeddings using pytorch nn.Embedding. Use vocabulary and map words to index and emojis to index. Entire vocabulary is fed as input to the NN but output of NN is either emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, eVocab_size, embedding_dim, context_size, hidden):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "        self.linear2 = nn.Linear(hidden, eVocab_size)\n",
    "\n",
    "    def compute_loss(self, predicted_vector, label):\n",
    "        return self.loss(predicted_vector, label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.activation(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = self.softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFfeatures(data):\n",
    "    trigrams = []\n",
    "    numEmpty = 0\n",
    "    total = 0\n",
    "    for text in data[\"words\"]:\n",
    "        currTri = []\n",
    "        for i in range(len(text) - 2):\n",
    "            predictWord = text[i+2]\n",
    "            if not RE_EMOJI.match(predictWord):\n",
    "                predictWord = empty\n",
    "                numEmpty += 1\n",
    "            total += 1\n",
    "            currTri.append([[text[i], text[i+1]], predictWord])\n",
    "        trigrams.append(currTri)\n",
    "    print(\"% empty:{}\".format(numEmpty / total))\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\% empty:0.7202584279710275\n"
     ]
    }
   ],
   "source": [
    "train_feats = FFfeatures(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for epoch:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaa3d527ee34b2aa6ea4b3feb649d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ab795c9c7d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed for epoch:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "model = NGramLanguageModeler(len(vocab), len(emojiVocab), EMBEDDING_DIM, CONTEXT_SIZE, 128)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Training started for epoch:{}\".format(epoch + 1))\n",
    "    random.shuffle(train_feats)\n",
    "    start_time = time.time()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    minibatch_size = 16\n",
    "    N = len(train_feats)\n",
    "    for minibatch_idx in tqdm(range(N // minibatch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = None\n",
    "        for idx in range(minibatch_size):\n",
    "            text = train_feats[minibatch_idx * minibatch_size + idx]\n",
    "            for context, target in text:\n",
    "                context_idx = torch.tensor([vocabIdx[w] for w in context], dtype=torch.long)\n",
    "                log_probs = model(context_idx)\n",
    "                idx_loss = model.compute_loss(log_probs, torch.tensor([eVocabIdx[target]]))\n",
    "                \n",
    "                if loss is None:\n",
    "                    loss = idx_loss\n",
    "                else:\n",
    "                    loss += idx_loss\n",
    "                predicted_label = torch.argmax(log_probs)\n",
    "                correct += int(predicted_label == eVocabIdx[target])\n",
    "                total += 1\n",
    "        loss = loss / minibatch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Training completed for epoch:{}\".format(epoch + 1))\n",
    "    print(\"Time for train:{}\".format(time.time() - start_time))\n",
    "    print(\"Accuracy:{}\".format(correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/LSTM \n",
    "\n",
    "+ https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py\n",
    "+ 2 models\n",
    "  + one to determine whether a word is an emoji (treated as a seq labelling task)\n",
    "  + one to determine which emoji it is (also treated as seq labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
