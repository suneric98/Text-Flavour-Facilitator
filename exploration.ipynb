{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "empty = \"<empty>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    emojiVocab.add(empty)\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)\n",
    "vocabIdx = {word : i for i, word in enumerate(vocab)}\n",
    "eVocabIdx = {emoji : i for i, emoji in enumerate(emojiVocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=19720, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "# Takes: dataset, word2vec model, and vocabulary from training\n",
    "# returns: list of tuples. First value is a torch of word embeddings for that sentence,\n",
    "# second value is the labels for each word\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'i', 'have', 'ğŸˆ¶', 'ğŸˆ¶', 'your', 'ğŸ‘‰', 'information', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'do', 'not', 'block', 'me', 'or', 'i', 'will', 'take', 'immediate', 'action', 'you', 'are', 'aaron', 'thompson', 'from', 'boston', 'massachusetts', 'and', 'you', 'are', 'commuting', 'a', 'crime', 'by', 'having', 'possession', 'of', 'child', 'ğŸ‘¶', 'ğŸ‘¶', 'ğŸ‘¶', 'pornography', 'i', 'will', 'go', 'public', 'ğŸš‹', 'ğŸš‹', 'with', 'these', 'screenshots', 'of', 'our', 'convorsarion', 'and', 'send', 'your', 'ğŸ‘‰', 'nude', 'photo', 'ğŸ¡', 'ğŸ¡', 'ğŸ¡', 'to', 'your', 'ğŸ‘‰', 'family', 'ğŸ‘ª', 'contact', 'if', 'i', 'do', 'not', 'receive', '20', 'dollar', 'in', 'the', 'next', 'â­', 'â­', 'â­', '20', 'minute', 'if', 'you', 'do', 'not', 'respond', 'within', '30', 'second', 'i', 'am', 'sharing', 'this', 'â¬†', 'with', 'the', 'proper', 'authority', 'and', 'family', 'ğŸ‘ª', 'contact', 'do', 'not', 'block', 'me', 'because', 'if', 'you', 'do', 'i', 'will', 'do', 'this', 'â¬†', 'immediately', 'send', '20', 'via', 'paypal', 'to', 'moneymangmailcom', 'your', 'ğŸ‘‰', 'ip', 'Â©', 'ï¸', 'Â©', 'ï¸', 'address', 'which', 'can', 'be', 'used', 'with', 'the', 'authority', 'to', 'track', 'you', 'further', 'is', 'ğŸˆ¶', '282982284', '</s>']\n",
      "[0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "tensor([[ 7.7518e-01, -2.8338e-01,  5.3663e-01,  ...,  6.2686e-01,\n",
      "         -1.1062e-01,  5.0254e-01],\n",
      "        [ 1.0140e+00, -8.7087e-01,  1.5880e+00,  ...,  1.4163e+00,\n",
      "          4.2022e-01,  1.2915e-01],\n",
      "        [ 9.6204e-01, -3.1399e-01,  1.4149e+00,  ...,  7.6088e-01,\n",
      "          2.9804e-01, -2.5840e-01],\n",
      "        ...,\n",
      "        [ 1.3624e+00, -2.6305e-01,  9.6897e-01,  ...,  1.3768e-01,\n",
      "          7.7685e-01, -8.5023e-01],\n",
      "        [ 8.2293e-03,  3.9757e-03,  6.4202e-03,  ...,  1.0446e-02,\n",
      "          9.3653e-04, -2.7997e-03],\n",
      "        [ 6.0487e-01,  1.8383e-01,  5.8120e-01,  ...,  1.3200e+00,\n",
      "         -5.7793e-01,  7.8856e-01]])\n",
      "['<s>', 'have', 'ğŸ˜±', 'you', 'ğŸ˜±', 'ever', 'ğŸ˜±', 'used', 'ğŸ˜±', 'the', 'ğŸ˜±', 'bathroom', 'ğŸ˜±', 'at', 'ğŸ˜±', 'night', 'ğŸ˜±', 'and', 'ğŸ˜±', 'accidentally', 'ğŸ˜±', 'urinated', 'ğŸ˜±', 'over', 'ğŸ˜±', 'your', 'ğŸ˜±', 'new', 'ğŸ˜±', 'toilet', 'ğŸ˜±', 'seat', 'ğŸ˜±', 'the', 'ğŸ˜±', 'wrath', 'ğŸ˜±', 'of', 'ğŸ˜±', 'your', 'ğŸ˜±', 'family', 'ğŸ˜±', 'member', 'ğŸ˜±', 'the', 'ğŸ˜±', 'next', 'ğŸ˜±', 'morning', 'ğŸ˜±', '</s>']\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 0.9620, -0.3140,  1.4149,  ...,  0.7609,  0.2980, -0.2584],\n",
      "        [ 1.9489,  0.2331, -1.0011,  ...,  0.2954, -0.2032,  0.9893],\n",
      "        ...,\n",
      "        [ 0.0989,  0.0278,  0.0687,  ...,  0.1423,  0.0127,  0.0242],\n",
      "        [ 1.9489,  0.2331, -1.0011,  ...,  0.2954, -0.2032,  0.9893],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n",
      "['<s>', 'i', 'just', 'had', 'ğŸ™‚', 'ğŸ˜†', 'the', 'displeasure', 'of', 'having', 'ğŸ¤¨', 'ğŸ˜«', 'another', 'mod', 'from', 'an', 'entirely', 'different', 'subreddit', 'ğŸ‘½', 'ğŸ’¬', 'message', 'ğŸ’¬', 'ğŸ’¬', 'ğŸ’¬', 'u', 'ğŸ‡ºğŸ‡¸', 'about', 'some', 'no', 'ğŸ˜£', 'ğŸ˜£', 'nut', 'ğŸ”©', 'ğŸ¥œ', 'scum', 'attacking', 'ğŸ‘Š', 'their', 'subreddit', 'ğŸ‘½', 'with', 'copypasta', 'yeah', 'ğŸ™Œ', 'that', 'â–¶', 'ï¸right', 'â–¶', 'ï¸', 'ğŸ‘‰', 'ğŸ‘‰', 'ğŸ‘‰', 'it', 'ğŸ‡®ğŸ‡¹', 'ğŸ‡®ğŸ‡¹', 'that', 'bword', 'brigading', 'this', 'â¬†', 'mod', 'detailed', 'to', 'me', 'how', 'ğŸ¤”', 'hard', 'it', 'ğŸ‡®ğŸ‡¹', 'is', 'ğŸˆ¶', 'to', 'â°', 'wake', 'â°', 'â°', 'in', 'the', 'morning', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'with', 'no', 'ğŸ‘', 'ğŸ‘', 'ğŸ˜£', 'job', 'ğŸ’¼', 'ğŸ’¼', 'or', 'prospect', 'in', 'life', 'ğŸ’“', 'warm', 'ğŸ”…', 'ğŸœ', 'up', 'ğŸ”', 'â˜', 'â˜', 'the', 'chicken', 'ğŸ”', 'ğŸ”', 'ğŸ”', 'ğŸ”', 'ğŸ”', 'tendies', 'log', 'onto', 'reddit', 'ğŸ‘½', 'com', 'and', 'begin', 'modding', 'only', 'to', 'find', 'ğŸ”', 'ğŸ”', 'ğŸ”', 'ğŸ”', 'out', 'ğŸ', 'ğŸ', 'that', 'some', 'societydwelling', 'jester', 'engaged', 'in', 'posting', 'ğŸš©', 'a', 'ğŸ…°', 'ï¸', 'fresh', 'â›²', 'copypasta', 'from', 'this', 'â¬†', 'subreddit', 'ğŸ‘½', 'ğŸ…°', 'ï¸as', 'ğŸ…°', 'ï¸', 'a', 'ğŸ…°', 'ï¸', 'ğŸ…°', 'ï¸', 'legitimate', 'post', 'ğŸš©', 'in', 'another', 'this', 'â¬†', 'mod', 'ğŸ’”', 'broke', 'ğŸ’”', 'into', 'ğŸ˜‚', 'tear', 'ğŸ˜‚', 'ğŸ˜‚', 'ğŸ˜‚', 'explaining', 'to', 'u', 'ğŸ‡»ğŸ‡®', 'how', 'ğŸ¤”', 'it', 'ğŸ‡®ğŸ‡¹', 'hard', 'enough', 'to', 'moderate', 'ğŸ…°', 'ï¸as', 'ğŸ…°', 'ï¸', 'it', 'ğŸ‡®ğŸ‡¹', 'is', 'ğŸˆ¶', 'and', 'ğŸ‡®ğŸ‡¹', 'it', 'ğŸ‡®ğŸ‡¹', 'doesnt', 'get', 'ğŸ‰', 'ğŸ‰', 'ğŸ‰', 'any', 'easier', 'due', 'to', 'these', 'heinous', 'act', 'ğŸ­', 'of', 'brigading', 'content', 'in', 'this', 'â¬†', 'subreddit', 'ğŸ‘½', 'is', 'ğŸˆ¶', 'for', 'your', 'ğŸ‘‰', 'sick', 'ğŸ˜·', 'ğŸ˜£', 'ğŸ˜£', 'ğŸ˜£', 'enjoyment', 'so', 'ğŸ†˜', 'dont', 'turn', 'it', 'ğŸ‡®ğŸ‡¹', 'around', 'and', 'discus', 'ğŸ“Œ', 'here', 'ğŸ“Œ', 'ğŸ‘ˆ', 'ğŸ‘ˆ', 'about', 'spamming', 'other', 'sub', 'because', 'thats', 'against', 'reddit', 'ğŸ‘½', 'ğŸš·', 'rule', 'ğŸš·', 'ğŸš·', 'ğŸš·', 'we', 'dont', 'encourage', 'brigading', 'and', 'although', 'we', 'cant', 'ğŸ‘®', 'police', 'ğŸ‘®', 'ğŸ‘®\\u200dâ™€ï¸', 'ğŸ‘®\\u200dâ™€ï¸', 'other', 'subreddits', 'you', 'will', 'be', 'banned', 'if', 'you', 'ğŸ“²', 'call', 'ğŸ“²', 'to', 'action', 'about', 'posting', 'ğŸš©', 'pasta', 'ğŸ', 'ğŸ', 'in', 'another', 'subreddit', 'ğŸ‘½', 'this', 'â¬†', 'includes', 'ğŸ”—', 'linking', 'ğŸ”—', 'to', 'freshly', 'made', 'brigading', 'post', 'ğŸ£', 'asking', 'ğŸ™', 'for', 'vote', 'â˜‘', 'ï¸', 'or', 'suggestion', 'to', 'upvote', 'a', 'ğŸ…°', 'ï¸', 'ğŸ…°', 'ï¸', 'freshly', 'made', 'shitpost', 'ğŸ‘Œ', 'ğŸ‘Œ', 'ğŸ‘Œ', 'through', 'ğŸ…°', 'ï¸a', 'ğŸ…°', 'ï¸', 'ğŸ‘¤', 'user', 'ğŸ‘¤', 'ğŸ‘¤', 'ğŸ‘¤', 'profile', 'some', 'of', 'the', 'subreddits', 'in', 'â”', 'question', 'â”', 'â“', 'where', 'ğŸ¤·', 'this', 'â¬†', 'seems', 'to', 'be', 'common', 'is', 'ramitheasshole', 'you', 'will', 'be', 'banned', 'here', 'ğŸˆ', 'ğŸ‘ˆ', 'ğŸ‘ˆ', 'if', 'reported', 'for', 'posting', 'ğŸš©', 'pasta', 'ğŸ', 'ğŸ', 'ğŸ', 'there', 'a', 'ğŸ…°', 'ï¸', 'new', 'ğŸ‡³ğŸ‡¨', 'ğŸ†•', 'ğŸ†•', 'post', 'ğŸ¤', 'so', 'you', 'need', 'to', 'ask', 'ğŸ™', 'ğŸ™', 'yourself', 'aita', 'for', 'pooping', 'ğŸ’©', 'on', 'ğŸ”›', 'ğŸ”›', 'ğŸ”›', 'the', 'homeless', 'and', 'brigading', 'other', 'subreddits', 'info', 'ğŸš®', 'ğŸš®', 'ğŸš®', 'for', 'the', 'first', 'ğŸ¥‡', 'ğŸ¥‡', 'part', 'ã€½', 'ï¸', 'but', 'definitely', 'yta', 'for', 'the', 'second', 'ğŸ¥ˆ', 'ğŸ¥ˆ', 'ğŸ¥ˆ', '</s>']\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 1.0140, -0.8709,  1.5880,  ...,  1.4163,  0.4202,  0.1292],\n",
      "        [ 0.8664, -0.3890,  0.9217,  ...,  0.9420,  0.2225,  0.2526],\n",
      "        ...,\n",
      "        [ 0.0605,  0.0579,  0.1442,  ...,  0.1547, -0.0091,  0.0127],\n",
      "        [ 0.0605,  0.0579,  0.1442,  ...,  0.1547, -0.0091,  0.0127],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'you', 'ğŸ‘‰', 'go', 'to', 'school', 'ğŸ«', 'but', 'i', 'ğŸ‘€', 'go', 'to', 'concert', 'ğŸ¤', 'ğŸµ', 'ğŸ¶', 'you', 'don', 'â€™', 't', 'ğŸš«', 'learn', 'ğŸ“š', 'ğŸ“–', 'âœ', 'ï¸', 'shit', 'ğŸ’©', 'compared', 'to', 'what', 'i', 'learn', 'ğŸ“š', 'ğŸ“–', 'âœ', 'ï¸', 'in', 'the', 'pit', 'ğŸƒ\\u200dâ™‚ï¸', 'â­•', 'ï¸', '</s>']\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 0.8660, -0.6831,  1.2369,  ...,  1.1319,  0.2700,  0.1876],\n",
      "        [ 1.2860,  0.2209,  0.5731,  ...,  1.2231,  0.3449,  0.7515],\n",
      "        ...,\n",
      "        [-0.0921,  0.1108, -0.0412,  ...,  0.0108,  0.0603, -0.1010],\n",
      "        [ 0.9478,  0.7808,  0.7707,  ..., -1.0421, -0.0534, -0.3491],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n",
      "['<s>', 'uh', 'ohh', 'stinkyy', 'ğŸ˜¨', 'ğŸ’©', 'pooop', 'ahhahhaha', 'pooopiees', 'funny', 'poopiees', 'lalalelelelelelelelele', 'funny', 'poo', 'ğŸ’©', 'ğŸ’©', 'ğŸ’©', 'poop', 'funny', 'weeeeeee', 'haha', 'yayyyy', 'for', 'poopy', 'good', 'poopy', 'poopy', 'funny', 'hahahaha', 'ha', 'ha', 'ğŸ’©', 'ğŸ’©', 'ğŸ’©', 'ğŸ’', 'poop', 'poop', 'poop', 'poop', 'poop', 'poop', 'poop', 'funnyyy', 'yaay', 'ğŸ’©', 'ğŸ’©', 'ğŸ’©', 'bad', 'bad', 'poop', 'hee', 'hee', 'hee', 'poo', 'ğŸ’©', 'poopy', 'yay', 'poop', 'make', 'me', 'happy', 'happy', 'hahahhahaha', 'uh', 'oh', 'i', 'think', 'i', 'made', 'a', 'poop', 'pooping', 'pant', 'ğŸ’©', 'ğŸ’©', 'no', 'diaper', 'thats', 'funny', 'hahahahaheha', 'woopsie', 'poopy', 'underwear', 'now', 'ğŸ’©', 'ğŸ’©', 'sskkekkshehe', 'we', 'want', 'poopies', 'we', 'want', 'poopies', 'he', 'h', 'ahahahahahaaa', 'ğŸ’©', 'ğŸ’©', 'hehahaha', 'wheeze', 'poo', 'o', 'poop', 'ğŸ’©', 'ğŸ’©', '</s>']\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 0.6558, -0.2026,  0.2564,  ...,  0.4339, -0.3151,  0.4096],\n",
      "        [ 0.0334,  0.0261,  0.0316,  ...,  0.0749, -0.0090,  0.0156],\n",
      "        ...,\n",
      "        [ 1.3618, -0.5078,  1.3886,  ...,  0.8624, -2.6677,  0.8706],\n",
      "        [ 1.3618, -0.5078,  1.3886,  ...,  0.8624, -2.6677,  0.8706],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n"
     ]
    }
   ],
   "source": [
    "sample = train.sample(5)\n",
    "for index,row in sample.iterrows():\n",
    "    print(row[\"words\"])\n",
    "    print(row[\"labels\"])\n",
    "    print(trainEmb[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building supervised models to predict next emoji\n",
    "\n",
    "+ RNN based architecture where we look at the hidden layer for every word\n",
    "  + Using hidden layer, predict if is an emoji and what emoji it is\n",
    "+ Asked TA from NLP class, they said this is similar to a language modelling problem where we only predict the set of emoji vocabulary\n",
    "  + Could also view as sequence labelling where tag is next emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward NN\n",
    "\n",
    "+ https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "+ Built using previous couple of words\n",
    "+ Feature is word embeddings using pytorch nn.Embedding. Use vocabulary and map words to index and emojis to index. Entire vocabulary is fed as input to the NN but output of NN is either emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, eVocab_size, embedding_dim, context_size, hidden):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "        self.linear2 = nn.Linear(hidden, eVocab_size)\n",
    "\n",
    "    def compute_loss(self, predicted_vector, label):\n",
    "        return self.loss(predicted_vector, label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.activation(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = self.softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFfeatures(data):\n",
    "    trigrams = []\n",
    "    for text in data[\"words\"]:\n",
    "        currTri = []\n",
    "        for i in range(len(text) - 2):\n",
    "            predictWord = text[i+2]\n",
    "            if not RE_EMOJI.match(predictWord):\n",
    "                predictWord = empty\n",
    "            currTri.append([[text[i], text[i+1]], predictWord])\n",
    "        trigrams.append(currTri)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_feats = FFfeatures(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for epoch:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c276ff73d3db4fbd9a9aa0d4ad6d1227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss cuda:0\n",
      "loss cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c6d242f8d2d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mtrainFF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-c6d242f8d2d8>\u001b[0m in \u001b[0;36mtrainFF\u001b[1;34m(epochs, cuda)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[0mcontext_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocabIdx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                     \u001b[1;31m#print('logprb ' + str(log_probs.device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0midx_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meVocabIdx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-15078b8daa83>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "\n",
    "def trainFF(epochs,cuda):\n",
    "    devC = None\n",
    "    if cuda and torch.cuda.is_available():\n",
    "        devC = torch.device('cuda')\n",
    "    else:\n",
    "        devC = torch.device('cpu')\n",
    "    \n",
    "    losses = []\n",
    "    model = NGramLanguageModeler(len(vocab), len(emojiVocab), EMBEDDING_DIM, CONTEXT_SIZE, 128).to(devC)\n",
    "    optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Training started for epoch:{}\".format(epoch + 1))\n",
    "        random.shuffle(train_feats)\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        minibatch_size = 16\n",
    "        \n",
    "        N = len(train_feats)\n",
    "        for minibatch_idx in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for idx in range(minibatch_size):\n",
    "                text = train_feats[minibatch_idx * minibatch_size + idx]\n",
    "                for context, target in text:\n",
    "                    context_idx = torch.tensor([vocabIdx[w] for w in context], dtype=torch.long, device=devC)\n",
    "                    log_probs = model(context_idx)\n",
    "                    idx_loss = model.compute_loss(log_probs, torch.tensor([eVocabIdx[target]], device=devC))\n",
    "                    loss += idx_loss\n",
    "                    predicted_label = torch.argmax(log_probs)\n",
    "                    correct += int(predicted_label == eVocabIdx[target])\n",
    "                    total += 1\n",
    "            loss = loss / minibatch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        losses.append(loss) #was total_loss, but thats undef?\n",
    "        print(\"Training completed for epoch:{}\".format(epoch + 1))\n",
    "        print(\"Time for train:{}\".format(time.time() - start_time))\n",
    "        print(\"Accuracy:{}\".format(correct / total))\n",
    "        \n",
    "trainFF(1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
