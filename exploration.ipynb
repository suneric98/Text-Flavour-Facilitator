{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "empty = \"<empty>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    emojiVocab.add(empty)\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)\n",
    "vocabIdx = {word : i for i, word in enumerate(vocab)}\n",
    "eVocabIdx = {emoji : i for i, emoji in enumerate(emojiVocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=19720, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "# Takes: dataset, word2vec model, and vocabulary from training\n",
    "# returns: list of tuples. First value is a torch of word embeddings for that sentence,\n",
    "# second value is the labels for each word\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'i', 'have', '🈶', '🈶', 'your', '👉', 'information', '💁', '💁', '💁', 'do', 'not', 'block', 'me', 'or', 'i', 'will', 'take', 'immediate', 'action', 'you', 'are', 'aaron', 'thompson', 'from', 'boston', 'massachusetts', 'and', 'you', 'are', 'commuting', 'a', 'crime', 'by', 'having', 'possession', 'of', 'child', '👶', '👶', '👶', 'pornography', 'i', 'will', 'go', 'public', '🚋', '🚋', 'with', 'these', 'screenshots', 'of', 'our', 'convorsarion', 'and', 'send', 'your', '👉', 'nude', 'photo', '🎡', '🎡', '🎡', 'to', 'your', '👉', 'family', '👪', 'contact', 'if', 'i', 'do', 'not', 'receive', '20', 'dollar', 'in', 'the', 'next', '⏭', '⏭', '⏭', '20', 'minute', 'if', 'you', 'do', 'not', 'respond', 'within', '30', 'second', 'i', 'am', 'sharing', 'this', '⬆', 'with', 'the', 'proper', 'authority', 'and', 'family', '👪', 'contact', 'do', 'not', 'block', 'me', 'because', 'if', 'you', 'do', 'i', 'will', 'do', 'this', '⬆', 'immediately', 'send', '20', 'via', 'paypal', 'to', 'moneymangmailcom', 'your', '👉', 'ip', '©', '️', '©', '️', 'address', 'which', 'can', 'be', 'used', 'with', 'the', 'authority', 'to', 'track', 'you', 'further', 'is', '🈶', '282982284', '</s>']\n",
      "[0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "tensor([[ 7.7518e-01, -2.8338e-01,  5.3663e-01,  ...,  6.2686e-01,\n",
      "         -1.1062e-01,  5.0254e-01],\n",
      "        [ 1.0140e+00, -8.7087e-01,  1.5880e+00,  ...,  1.4163e+00,\n",
      "          4.2022e-01,  1.2915e-01],\n",
      "        [ 9.6204e-01, -3.1399e-01,  1.4149e+00,  ...,  7.6088e-01,\n",
      "          2.9804e-01, -2.5840e-01],\n",
      "        ...,\n",
      "        [ 1.3624e+00, -2.6305e-01,  9.6897e-01,  ...,  1.3768e-01,\n",
      "          7.7685e-01, -8.5023e-01],\n",
      "        [ 8.2293e-03,  3.9757e-03,  6.4202e-03,  ...,  1.0446e-02,\n",
      "          9.3653e-04, -2.7997e-03],\n",
      "        [ 6.0487e-01,  1.8383e-01,  5.8120e-01,  ...,  1.3200e+00,\n",
      "         -5.7793e-01,  7.8856e-01]])\n",
      "['<s>', 'have', '😱', 'you', '😱', 'ever', '😱', 'used', '😱', 'the', '😱', 'bathroom', '😱', 'at', '😱', 'night', '😱', 'and', '😱', 'accidentally', '😱', 'urinated', '😱', 'over', '😱', 'your', '😱', 'new', '😱', 'toilet', '😱', 'seat', '😱', 'the', '😱', 'wrath', '😱', 'of', '😱', 'your', '😱', 'family', '😱', 'member', '😱', 'the', '😱', 'next', '😱', 'morning', '😱', '</s>']\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 0.9620, -0.3140,  1.4149,  ...,  0.7609,  0.2980, -0.2584],\n",
      "        [ 1.9489,  0.2331, -1.0011,  ...,  0.2954, -0.2032,  0.9893],\n",
      "        ...,\n",
      "        [ 0.0989,  0.0278,  0.0687,  ...,  0.1423,  0.0127,  0.0242],\n",
      "        [ 1.9489,  0.2331, -1.0011,  ...,  0.2954, -0.2032,  0.9893],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n",
      "['<s>', 'i', 'just', 'had', '🙂', '😆', 'the', 'displeasure', 'of', 'having', '🤨', '😫', 'another', 'mod', 'from', 'an', 'entirely', 'different', 'subreddit', '👽', '💬', 'message', '💬', '💬', '💬', 'u', '🇺🇸', 'about', 'some', 'no', '😣', '😣', 'nut', '🔩', '🥜', 'scum', 'attacking', '👊', 'their', 'subreddit', '👽', 'with', 'copypasta', 'yeah', '🙌', 'that', '▶', '️right', '▶', '️', '👉', '👉', '👉', 'it', '🇮🇹', '🇮🇹', 'that', 'bword', 'brigading', 'this', '⬆', 'mod', 'detailed', 'to', 'me', 'how', '🤔', 'hard', 'it', '🇮🇹', 'is', '🈶', 'to', '⏰', 'wake', '⏰', '⏰', 'in', 'the', 'morning', '🌞', '🌞', '🌞', 'with', 'no', '👎', '👎', '😣', 'job', '💼', '💼', 'or', 'prospect', 'in', 'life', '💓', 'warm', '🔅', '🏜', 'up', '🔝', '☝', '☝', 'the', 'chicken', '🐔', '🐔', '🐔', '🐔', '🐔', 'tendies', 'log', 'onto', 'reddit', '👽', 'com', 'and', 'begin', 'modding', 'only', 'to', 'find', '🔍', '🔍', '🔍', '🔍', 'out', '🏎', '🏍', 'that', 'some', 'societydwelling', 'jester', 'engaged', 'in', 'posting', '🚩', 'a', '🅰', '️', 'fresh', '⛲', 'copypasta', 'from', 'this', '⬆', 'subreddit', '👽', '🅰', '️as', '🅰', '️', 'a', '🅰', '️', '🅰', '️', 'legitimate', 'post', '🚩', 'in', 'another', 'this', '⬆', 'mod', '💔', 'broke', '💔', 'into', '😂', 'tear', '😂', '😂', '😂', 'explaining', 'to', 'u', '🇻🇮', 'how', '🤔', 'it', '🇮🇹', 'hard', 'enough', 'to', 'moderate', '🅰', '️as', '🅰', '️', 'it', '🇮🇹', 'is', '🈶', 'and', '🇮🇹', 'it', '🇮🇹', 'doesnt', 'get', '🉐', '🉐', '🉐', 'any', 'easier', 'due', 'to', 'these', 'heinous', 'act', '🎭', 'of', 'brigading', 'content', 'in', 'this', '⬆', 'subreddit', '👽', 'is', '🈶', 'for', 'your', '👉', 'sick', '😷', '😣', '😣', '😣', 'enjoyment', 'so', '🆘', 'dont', 'turn', 'it', '🇮🇹', 'around', 'and', 'discus', '📌', 'here', '📌', '👈', '👈', 'about', 'spamming', 'other', 'sub', 'because', 'thats', 'against', 'reddit', '👽', '🚷', 'rule', '🚷', '🚷', '🚷', 'we', 'dont', 'encourage', 'brigading', 'and', 'although', 'we', 'cant', '👮', 'police', '👮', '👮\\u200d♀️', '👮\\u200d♀️', 'other', 'subreddits', 'you', 'will', 'be', 'banned', 'if', 'you', '📲', 'call', '📲', 'to', 'action', 'about', 'posting', '🚩', 'pasta', '🍝', '🍝', 'in', 'another', 'subreddit', '👽', 'this', '⬆', 'includes', '🔗', 'linking', '🔗', 'to', 'freshly', 'made', 'brigading', 'post', '🏣', 'asking', '🙏', 'for', 'vote', '☑', '️', 'or', 'suggestion', 'to', 'upvote', 'a', '🅰', '️', '🅰', '️', 'freshly', 'made', 'shitpost', '👌', '👌', '👌', 'through', '🅰', '️a', '🅰', '️', '👤', 'user', '👤', '👤', '👤', 'profile', 'some', 'of', 'the', 'subreddits', 'in', '❔', 'question', '❔', '❓', 'where', '🤷', 'this', '⬆', 'seems', 'to', 'be', 'common', 'is', 'ramitheasshole', 'you', 'will', 'be', 'banned', 'here', '🈁', '👈', '👈', 'if', 'reported', 'for', 'posting', '🚩', 'pasta', '🍝', '🍝', '🍝', 'there', 'a', '🅰', '️', 'new', '🇳🇨', '🆕', '🆕', 'post', '🏤', 'so', 'you', 'need', 'to', 'ask', '🙏', '🙏', 'yourself', 'aita', 'for', 'pooping', '💩', 'on', '🔛', '🔛', '🔛', 'the', 'homeless', 'and', 'brigading', 'other', 'subreddits', 'info', '🚮', '🚮', '🚮', 'for', 'the', 'first', '🥇', '🥇', 'part', '〽', '️', 'but', 'definitely', 'yta', 'for', 'the', 'second', '🥈', '🥈', '🥈', '</s>']\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 1.0140, -0.8709,  1.5880,  ...,  1.4163,  0.4202,  0.1292],\n",
      "        [ 0.8664, -0.3890,  0.9217,  ...,  0.9420,  0.2225,  0.2526],\n",
      "        ...,\n",
      "        [ 0.0605,  0.0579,  0.1442,  ...,  0.1547, -0.0091,  0.0127],\n",
      "        [ 0.0605,  0.0579,  0.1442,  ...,  0.1547, -0.0091,  0.0127],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'you', '👉', 'go', 'to', 'school', '🏫', 'but', 'i', '👀', 'go', 'to', 'concert', '🎤', '🎵', '🎶', 'you', 'don', '’', 't', '🚫', 'learn', '📚', '📖', '✏', '️', 'shit', '💩', 'compared', 'to', 'what', 'i', 'learn', '📚', '📖', '✏', '️', 'in', 'the', 'pit', '🏃\\u200d♂️', '⭕', '️', '</s>']\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 0.8660, -0.6831,  1.2369,  ...,  1.1319,  0.2700,  0.1876],\n",
      "        [ 1.2860,  0.2209,  0.5731,  ...,  1.2231,  0.3449,  0.7515],\n",
      "        ...,\n",
      "        [-0.0921,  0.1108, -0.0412,  ...,  0.0108,  0.0603, -0.1010],\n",
      "        [ 0.9478,  0.7808,  0.7707,  ..., -1.0421, -0.0534, -0.3491],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n",
      "['<s>', 'uh', 'ohh', 'stinkyy', '😨', '💩', 'pooop', 'ahhahhaha', 'pooopiees', 'funny', 'poopiees', 'lalalelelelelelelelele', 'funny', 'poo', '💩', '💩', '💩', 'poop', 'funny', 'weeeeeee', 'haha', 'yayyyy', 'for', 'poopy', 'good', 'poopy', 'poopy', 'funny', 'hahahaha', 'ha', 'ha', '💩', '💩', '💩', '🐒', 'poop', 'poop', 'poop', 'poop', 'poop', 'poop', 'poop', 'funnyyy', 'yaay', '💩', '💩', '💩', 'bad', 'bad', 'poop', 'hee', 'hee', 'hee', 'poo', '💩', 'poopy', 'yay', 'poop', 'make', 'me', 'happy', 'happy', 'hahahhahaha', 'uh', 'oh', 'i', 'think', 'i', 'made', 'a', 'poop', 'pooping', 'pant', '💩', '💩', 'no', 'diaper', 'thats', 'funny', 'hahahahaheha', 'woopsie', 'poopy', 'underwear', 'now', '💩', '💩', 'sskkekkshehe', 'we', 'want', 'poopies', 'we', 'want', 'poopies', 'he', 'h', 'ahahahahahaaa', '💩', '💩', 'hehahaha', 'wheeze', 'poo', 'o', 'poop', '💩', '💩', '</s>']\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "tensor([[ 0.7752, -0.2834,  0.5366,  ...,  0.6269, -0.1106,  0.5025],\n",
      "        [ 0.6558, -0.2026,  0.2564,  ...,  0.4339, -0.3151,  0.4096],\n",
      "        [ 0.0334,  0.0261,  0.0316,  ...,  0.0749, -0.0090,  0.0156],\n",
      "        ...,\n",
      "        [ 1.3618, -0.5078,  1.3886,  ...,  0.8624, -2.6677,  0.8706],\n",
      "        [ 1.3618, -0.5078,  1.3886,  ...,  0.8624, -2.6677,  0.8706],\n",
      "        [ 0.6049,  0.1838,  0.5812,  ...,  1.3200, -0.5779,  0.7886]])\n"
     ]
    }
   ],
   "source": [
    "sample = train.sample(5)\n",
    "for index,row in sample.iterrows():\n",
    "    print(row[\"words\"])\n",
    "    print(row[\"labels\"])\n",
    "    print(trainEmb[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building supervised models to predict next emoji\n",
    "\n",
    "+ RNN based architecture where we look at the hidden layer for every word\n",
    "  + Using hidden layer, predict if is an emoji and what emoji it is\n",
    "+ Asked TA from NLP class, they said this is similar to a language modelling problem where we only predict the set of emoji vocabulary\n",
    "  + Could also view as sequence labelling where tag is next emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward NN\n",
    "\n",
    "+ https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "+ Built using previous couple of words\n",
    "+ Feature is word embeddings using pytorch nn.Embedding. Use vocabulary and map words to index and emojis to index. Entire vocabulary is fed as input to the NN but output of NN is either emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, eVocab_size, embedding_dim, context_size, hidden):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "        self.linear2 = nn.Linear(hidden, eVocab_size)\n",
    "\n",
    "    def compute_loss(self, predicted_vector, label):\n",
    "        return self.loss(predicted_vector, label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.activation(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = self.softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFfeatures(data):\n",
    "    trigrams = []\n",
    "    for text in data[\"words\"]:\n",
    "        currTri = []\n",
    "        for i in range(len(text) - 2):\n",
    "            predictWord = text[i+2]\n",
    "            if not RE_EMOJI.match(predictWord):\n",
    "                predictWord = empty\n",
    "            currTri.append([[text[i], text[i+1]], predictWord])\n",
    "        trigrams.append(currTri)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_feats = FFfeatures(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for epoch:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c276ff73d3db4fbd9a9aa0d4ad6d1227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss cuda:0\n",
      "loss cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c6d242f8d2d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mtrainFF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-c6d242f8d2d8>\u001b[0m in \u001b[0;36mtrainFF\u001b[1;34m(epochs, cuda)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[0mcontext_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocabIdx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                     \u001b[1;31m#print('logprb ' + str(log_probs.device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0midx_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meVocabIdx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-15078b8daa83>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "\n",
    "def trainFF(epochs,cuda):\n",
    "    devC = None\n",
    "    if cuda and torch.cuda.is_available():\n",
    "        devC = torch.device('cuda')\n",
    "    else:\n",
    "        devC = torch.device('cpu')\n",
    "    \n",
    "    losses = []\n",
    "    model = NGramLanguageModeler(len(vocab), len(emojiVocab), EMBEDDING_DIM, CONTEXT_SIZE, 128).to(devC)\n",
    "    optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Training started for epoch:{}\".format(epoch + 1))\n",
    "        random.shuffle(train_feats)\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        minibatch_size = 16\n",
    "        \n",
    "        N = len(train_feats)\n",
    "        for minibatch_idx in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for idx in range(minibatch_size):\n",
    "                text = train_feats[minibatch_idx * minibatch_size + idx]\n",
    "                for context, target in text:\n",
    "                    context_idx = torch.tensor([vocabIdx[w] for w in context], dtype=torch.long, device=devC)\n",
    "                    log_probs = model(context_idx)\n",
    "                    idx_loss = model.compute_loss(log_probs, torch.tensor([eVocabIdx[target]], device=devC))\n",
    "                    loss += idx_loss\n",
    "                    predicted_label = torch.argmax(log_probs)\n",
    "                    correct += int(predicted_label == eVocabIdx[target])\n",
    "                    total += 1\n",
    "            loss = loss / minibatch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        losses.append(loss) #was total_loss, but thats undef?\n",
    "        print(\"Training completed for epoch:{}\".format(epoch + 1))\n",
    "        print(\"Time for train:{}\".format(time.time() - start_time))\n",
    "        print(\"Accuracy:{}\".format(correct / total))\n",
    "        \n",
    "trainFF(1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
