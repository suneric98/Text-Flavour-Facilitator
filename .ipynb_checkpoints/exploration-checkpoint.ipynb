{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=28845, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'âš ', 'bruh', 'âš ', 'âš ', 'bruh', 'âš ', 'âš ', 'bruh', 'âš ', 'bÌ¶ÌÌÍ‹Í‹Í‹Ì§Ì±ÌœÌ¤Ì±Ì¦ÍˆÍ‰rÌ¶Ì€Í‹Í†Í’Í˜Ì¾Ì‚ÌŠÌŠÍÍ€Ì¥Ì¹ÍšÍ•ÍšÌ Ì–Ì£ÍšuÌµÍ‚ÌƒÌ…Ì„Ì¿Ì¾ÌŒÍÌÌ•Ì½Ì€Ì½Ì‚ÌŸÌªÍÌ¤Ì¯Ì»ÌÌÍ“ÍÌ±Í”uÌ·ÌšÌŒÍ›Í†ÌÍ›Í‘Ì¿Í„Ì‚ÌšÌ—Ì¯ÍÌ²Ì­Ì«Í…Í™uÌ´Ì¾ÌÍŒÌ€ÍŒÍ’Ì€Í’Í†Í„ÍŠÌ‡Ì¼Ì¼ÍÌ»Ì²Í‡Ì¬uÌ´Ì‹Ì¿Í Ì„ÌˆÌ¿ÍÍ„Ì°Ì¬Ì®Ì¤Ì¼ÌªÌ³ÌºÍ•ÌœÌ²Ì¦Ì–uÌ¶Ì‘ÌƒÌ‡Ì‹Ì›ÍÌ½Í˜Í•ÍˆÍœÌ»ÍšÍ–Í™Í•Ì«ÌœuÌ¸Í„ÌÍ›ÌºÌ˜Ì¨Í…uÌ¸ÌŒÌƒÌˆÍŠÌŠÌ’Ì…Ì»ÍˆÌœÌ¨Í“Ì«ÍÌ«Í™Ì–ÌŸÌ¦Í…uÌ´Ì„Ì‚ÌˆÍ‚Í˜Ì‚ÌÍŒÌ°Í‰ÍšÍ”Í•Í•Ì™Ì­ÍšÌ Ì®uÌ´ÍÍ—ÍŠÌ€Ì’Ì¥Ì¥Ì®Í”Ì¡Í•Ì¤Ì™Í”Ì¹uÌ¶Í—Í€ÌÍ„ÌÌ°Ì­Ì±Í–Í“Ì Í™ÍœuÌ·Ì¿Ì•ÍÍŒÌŒÌ Ì®Í“ÍˆÍ”ÍÌ¯Í”Ì¨Ì–Ì³Ì˜uÌ´Í’ÌˆÌ‘Í ÍÌŒÌ‹Í’Ì›Í›Ì•Í’ÌÌ’Ì¢Ì¯Í“Ì Ì²Ì¤Í™ÌÍ…ÍšÌ¬Ì¤Í“uÌ¶Ì‘Í›Ì‰Ì„Ì¾Í‚Ì¾Ì‡Ì…Í…ÌÍ…ÍÌ¢Ì¤Ì­Ì³Ì«Ì—Ì°uÌ¶Ì€ÌŠÌ‰Ì¾ÌªÌ˜ÍšÍ‡Ì—Ì¥Ì˜Í‡Ì¡ÍšÌ±Ì–uÌ¶Ì„Í‰Í–uÌ¸Ì¾Í„Í˜Í„Ì‘ÌˆÌ’ÌºÌ°ÌÌ¢Í…Ì»Ì¢Ì±Í‡Í‰Ì˜uÌ·ÍŠÍÌ¢Í”Í‰Ì¡ÌuÌ¸Í‹ÌÌ½Í‹ÌÌÌÍ‹Ì…ÌÌ›Í†Í„Ì„Ì®Ì©ÌÍ™ÌªÌÌ®Ì»Ì¤Ì¤uÌµÍŠÌ•Í‡ÌÍ“ÍÌ»Ì¢ÌºÌ«Í™Ì§Ì˜ÍÌÌ—uÌµÌ›ÍÌ uÌ¸Ì•ÌÌ¬Ì¼Ì±Ì±Ì©Í”Ì£ÌÌ§Í”ÌŸÌœÌÍ‡uÌµÌ¿Í„Ì”Ì’Ì›ÍƒÌ˜Ì©Í‡Ì Ì»Í™Í–Ì—Ì³ÌœhÌ·Ì†Í‚Í‘ÍÍ€Ì…Ì„Ì‚Ì‚Ì¾ÍŠÍƒÍŠÌ“ÍÍ“ÌªÍ‡Ì¹Ì­hÌ·Ì¿Ì›Í—Ì€ÌƒÍ‹Ì›Í‘Ì›ÌÍ Í‹Í‘Ì’ÍœÌ™Í…Í‰Ì¡hÌ´ÌŠÍ†Ì¿Ì‹Ì…Ì¿Ì“Ì‘Í‚Ì‹Ì„Í’Ì‡Ì§ÍœÌ©hÌ¸Ì•Ì‰Í›ÌŒÌ„Ì“Ì½Ì«Í•Ì¢Ì¦Ì–hÌ¸Ì‹ÌÌƒÌ‰ÍÍ„Í›Ì²ÍÌ©hÌ¸Í Ì•Ì‡Ì…ÌÍÌ¢Í”Ì˜Ì¡Ì®Ì³Ì¯Ì¯hÌ´Ì…Ì†ÍÍÌ’ÍÍ–Ì°ÍÌ£Ì±ÌÌÌŸÌ™Ì¯Ì±Í–Í“Ì­hÌ´Ì‰Ì«Ì§Í…Ì­Ì®Ì¦ÌºÌ©Í‰', 'the', 'ğŸ‘®', 'department', 'of', 'ğŸ ', 'homeland', 'ğŸ—½', 'security', 'ğŸš”', 'ha', 'issued', 'a', 'ğŸ…±', 'ruh', 'moment', 'âš ', 'warning', 'ğŸš§', 'for', 'the', 'following', 'district', 'ligma', 'sugma', 'ğŸ…±', 'ofa', 'and', 'sugondese', 'numerous', 'instance', 'of', 'ğŸ…±', 'ruh', 'moment', 'ğŸ…±', 'eing', 'triggered', 'by', 'ğŸ‘€', 'cringe', 'ğŸ˜¬', 'normies', 'ğŸš½', 'have', 'â°', 'recently', 'ğŸ•‘', 'occurred', 'across', 'the', 'ğŸŒ', 'continental', 'ğŸ‡ºğŸ‡¸', 'united', 'state', 'ğŸ‡ºğŸ‡¸', 'these', 'individual', 'are', 'ğŸ…±', 'elieved', 'to', 'ğŸ…±', 'e', 'highly', 'ğŸ”«', 'dangerous', 'ğŸ”ª', 'and', 'should', 'ğŸš«', 'not', 'âŒ', 'ğŸ…±', 'e', 'approached', 'citizen', 'are', 'instructed', 'to', 'remain', 'inside', 'and', 'ğŸ”’', 'lock', 'their', 'ğŸšª', 'door', 'under', 'âŒ', 'no', 'â›”', 'circumstance', 'should', 'any', 'citizen', 'ğŸ™Š', 'say', 'bruh', 'in', 'reaction', 'to', 'an', 'action', 'performed', 'ğŸ…±', 'y', 'a', 'cringe', 'ğŸ˜¬', 'normie', 'ğŸš½', 'and', 'should', 'store', 'the', 'following', 'item', 'in', 'a', 'secure', 'ğŸ”’', 'location', 'jahcoins', 'ğŸ’¶', 'vbucks', 'ğŸ’´', 'gekyumes', 'foreskin', 'ğŸ†', 'poop', 'ğŸ’©', 'sock', 'juul', 'ğŸ’­', 'pod', 'ball', 'ğŸ’', 'crusher', 'and', 'dip', 'remain', 'tuned', 'for', 'further', 'instruction', 'âš ', 'bruh', 'âš ', 'âš ', 'bruh', 'âš ', 'âš ', 'bruh', 'âš ', '</s>']\n",
      "[1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "(tensor([[-0.6602,  0.9093, -0.5563,  ...,  0.2494, -0.5998, -0.1178],\n",
      "        [ 0.1864, -0.6238, -0.2801,  ...,  0.4462,  0.1219,  0.5782],\n",
      "        [ 0.1696,  0.0276, -0.3821,  ...,  0.0734, -0.0254,  0.0167],\n",
      "        ...,\n",
      "        [ 0.1696,  0.0276, -0.3821,  ...,  0.0734, -0.0254,  0.0167],\n",
      "        [ 0.1864, -0.6238, -0.2801,  ...,  0.4462,  0.1219,  0.5782],\n",
      "        [ 0.1688,  0.7186,  0.0021,  ..., -0.1241, -0.2070,  0.5638]]), [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(train[\"words\"][0])\n",
    "print(train[\"labels\"][0])\n",
    "print(trainEmb[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
