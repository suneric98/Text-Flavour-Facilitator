{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=28845, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "# Takes: dataset, word2vec model, and vocabulary from training\n",
    "# returns: list of tuples. First value is a torch of word embeddings for that sentence,\n",
    "# second value is the labels for each word\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lol', 'ğŸ˜‚', 'if', 'i', 'ever', 'had', 'any', 'homosexual', 'ğŸ¤¢', 'child', 'ğŸ‘¶', 'know', 'what', 'i', 'â€™', 'd', 'do', 'ğŸ¤”', 'nothing', 'i', 'â€™', 'd', 'respect', 'âœŠ', 'their', 'choice', 'to', 'become', 'homosexual', 'ğŸ¤¢', 'just', 'like', 'i', 'respected', 'âœŠ', 'my', 'retard', 'cousin', 'bilo', 'â€™', 's', 'sex', 'change', 'to', 'a', 'gay', 'tranny', 'alpaca', 'ğŸ¦™', 'and', 'why', 'ğŸ¤”', 'because', 'ğŸ’\\u200dâ™€ï¸', 'i', 'â€™', 'm', 'not', 'a', 'homophobic', 'old', 'white', 'male', 'ğŸ˜‚', 'or', 'the', 'average', 'reddit', 'user', 'ğŸ¤·\\u200dâ™‚ï¸', 'i', 'am', 'chad', 'ğŸ˜', 'i', 'respect', 'âœŠ', 'people', 'â€™', 's', 'choice', 'a', 'individual', 'and', 'sex', 'the', 'woman', 'ğŸ‘©\\u200dğŸ¦³', '</s>']\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-0.9994,  0.0748, -0.3478,  ...,  0.7569,  0.0692, -0.1553],\n",
      "        [-2.2077,  0.5740, -1.6367,  ...,  2.1574, -0.5370, -0.5692],\n",
      "        ...,\n",
      "        [-0.5951, -0.1858, -0.5898,  ...,  0.6699,  0.0998,  0.0808],\n",
      "        [-0.0093,  0.0291, -0.3190,  ...,  0.1398, -0.0721, -0.0136],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'you', 'stupid', 'ğŸ¤¡', 'as', 'black', 'ğŸ‘¸ğŸ¾', 'bitch', 'ğŸ¶', 'stop', 'ğŸ™…\\u200dâ™€ï¸', 'screenshotting', 'ğŸ–¥', 'my', 'story', 'ğŸ“–', 'or', 'say', 'goodbye', 'ğŸ™‹ğŸ»\\u200dâ™€ï¸', 'to', 'you', 'last', 'day', 'ğŸ‘¿', 'this', 'is', 'ğŸ§–\\u200dâ™‚ï¸', 'the', 'end', 'âŒ', 'your', 'time', 'â²', 'ha', 'come', 'âŒ›', 'ï¸', 'it', 'a', 'wrap', 'ğŸ', 'the', 'clock', 'â°', 'ha', 'struck', 'ğŸ•›', 'midnight', 'ğŸŒŒ', 'time', 'ğŸ””', 'to', 'go', 'ğŸ”š', 'time', 'ğŸ•˜', 'to', 'pack', 'ğŸ§°', 'it', 'all', 'in', 'ğŸ“²', 'youve', 'had', 'a', 'good', 'run', 'ğŸƒ\\u200dâ™‚ï¸', 'youve', 'lived', 'ğŸ', 'quite', 'a', 'life', 'ğŸ«', 'youve', 'done', 'ğŸ', 'this', 'ğŸ„ğŸ¾\\u200dâ™€ï¸', 'and', 'that', 'ğŸ¤¹ğŸ¼\\u200dâ™‚ï¸', 'youve', 'loved', 'â¤', 'ï¸', 'lost', 'ğŸ’”', 'and', 'loved', 'ğŸ’•', 'some', 'more', 'ğŸ’', 'youve', 'accumulated', 'ğŸ“¸', 'quite', 'a', 'collection', 'ğŸ›’', 'of', 'memory', 'ğŸ—', 'about', 'people', 'ğŸ¤±', 'place', 'ğŸ', 'and', 'thing', 'ğŸ§¸', 'take', 'ğŸ•', 'a', 'moment', 'to', 'reflect', 'ğŸ”®', 'on', 'them', 'ğŸ’', 'all', 'allow', 'the', 'ğŸ–¼', 'entire', 'tableau', 'ğŸ¦', 'of', 'your', 'time', 'ğŸ•¦', 'here', 'on', 'earth', 'ğŸŒ', 'to', 'flash', 'ğŸ‡', 'before', 'your', 'eye', 'ğŸ‘€', 'aquire', 'a', 'final', 'ğŸ’¤', 'overwhelming', 'â€¼', 'ï¸', 'sense', 'â˜º', 'ï¸', 'of', 'peace', 'â˜®', 'ï¸', 'know', 'ğŸ§ ', 'that', 'everything', 'ğŸŒ', 'will', 'be', 'okay', 'ğŸ‘', 'look', 'into', 'ğŸ‘', 'that', 'approaching', 'tunnel', 'ğŸšŠ', 'of', 'light', 'â˜€', 'ï¸', 'reach', 'out', 'ğŸ•º', 'your', 'hand', 'ğŸ‘‹ğŸ½', 'to', 'jesus', 'â›ª', 'ï¸', 'be', 'struck', 'ğŸ’¥', 'by', 'the', 'ğŸŒŸ', 'presence', 'of', 'ğŸ™Œ', 'god', 'âš¡', 'ï¸', 'face', 'ğŸ‘¤', 'your', 'own', 'â˜ ', 'ï¸', 'mortality', 'ğŸ¥€', 'take', 'it', 'ğŸ˜¤', 'like', 'a', 'ğŸ§”ğŸ¼', 'man', 'you', 'ğŸ‘ˆ', 'have', 'little', 'ğŸ‘Œ', 'choice', 'now', 'might', 'a', 'well', 'ğŸ¤·\\u200dâ™€ï¸', 'accept', 'ğŸ˜Œ', 'the', 'way', 'thing', 'ğŸ’…ğŸ¿', 'are', 'you', 'dead', 'ğŸ‘»', '</s>']\n",
      "[0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-1.5767, -1.1055, -0.6313,  ...,  1.8308,  0.5306, -0.1961],\n",
      "        [-0.9561, -0.2979, -0.3954,  ...,  0.8991,  0.1773, -0.2182],\n",
      "        ...,\n",
      "        [-0.7522, -0.1611, -0.2687,  ...,  0.7636, -0.0055,  0.0373],\n",
      "        [-0.6412,  0.9150, -0.6565,  ...,  0.3183, -0.4655, -0.2694],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'this', 'world', 'ğŸŒ', 'is', 'rotten', 'ğŸ¤¢', 'ğŸ˜¤', 'and', 'those', 'who', 'are', 'making', 'it', 'rot', 'ğŸ˜±', 'ğŸ˜·', 'ğŸ˜°', 'deserve', 'to', 'die', 'ğŸ˜¡', 'ğŸ’€', 'ğŸ”ª', 'someone', 'ha', 'to', 'do', 'it', 'so', 'why', 'not', 'me', 'ğŸ¤”', 'ğŸ¤”', 'even', 'if', 'it', 'mean', 'sacrificing', 'ğŸ”ª', 'my', 'own', 'mind', 'ğŸ§ ', 'ğŸ˜±', 'and', 'soul', 'ğŸ‘»', 'it', 'worth', 'it', 'ğŸ‘', 'ğŸ™Œ', 'because', 'the', 'world', 'ğŸ˜', 'cant', 'go', 'on', 'like', 'this', 'ğŸ˜”', 'ğŸ˜¤', 'ğŸ˜£', 'i', 'wonder', 'ğŸ¤”', 'ğŸ‘€', 'what', 'if', 'someone', 'else', 'ğŸ™‡\\u200dâ™‚ï¸', 'ğŸ‘¨', 'had', 'picked', 'ğŸ¤š', 'up', 'this', 'notebook', 'ğŸ“”', 'âœ', 'is', 'there', 'anyone', 'ğŸ˜°', 'out', 'there', 'other', 'than', 'me', 'ğŸ™…\\u200dâ™‚ï¸', 'whod', 'be', 'willing', 'to', 'eliminate', 'the', 'vermin', 'ğŸ˜’', 'ğŸ', 'ğŸ€', 'from', 'the', 'world', 'ğŸ¤”', 'ğŸ¤·\\u200dâ™‚ï¸', 'if', 'i', 'dont', 'do', 'it', 'ğŸ‘Š', 'then', 'who', 'will', 'ğŸ˜§', 'thats', 'just', 'it', 'there', 'no', 'one', 'ğŸ™„', 'ğŸš«', 'ğŸ™…\\u200dâ™‚ï¸', 'but', 'i', 'can', 'do', 'it', 'â˜', 'ï¸', 'ğŸ¤', 'in', 'fact', 'im', 'the', 'only', 'one', 'who', 'can', 'ğŸ˜‹', 'ğŸ˜œ', 'ill', 'do', 'it', 'ğŸ˜‰', 'ğŸ˜™', 'using', 'the', 'death', 'note', 'â˜ ', 'ï¸', 'ğŸ“”', 'ill', 'change', 'the', 'world', 'ğŸ˜‹', 'ğŸ˜«', 'ğŸ‘ŒğŸ»', 'ğŸ¤˜ğŸ»', '</s>']\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-2.4187, -1.6323,  0.1832,  ...,  1.5042,  0.2358,  0.7519],\n",
      "        [-1.1427, -0.4600, -0.2972,  ...,  1.3303, -0.3635,  0.0958],\n",
      "        ...,\n",
      "        [-0.0134,  0.1986, -0.3422,  ...,  0.2392, -0.1800, -0.1248],\n",
      "        [ 0.1746,  0.1771, -0.3954,  ...,  0.1045, -0.1188, -0.0163],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'story', 'time', 'ğŸ•›', 'ğŸ•›', 'sister', 'ğŸ’\\u200dâ™€ï¸', 'ğŸ’\\u200dâ™€ï¸', 'so', 'basically', 'i', 'wa', 'in', 'class', 'listening', 'to', 'billie', 'eilish', 'â¤', 'ï¸', 'â¤', 'ï¸and', 'my', 'headphone', 'got', 'unplugged', 'ğŸ”Œ', 'ğŸ”Œ', 'ğŸ”Œ', 'ğŸ˜©', 'ğŸ˜©', 'and', 'it', 'played', 'bad', 'guy', 'ğŸ¥º', 'ğŸ¥º', 'out', 'loud', 'ğŸ”Š', 'ğŸ”Š', 'so', 'anyway', 'it', 'wa', 'playing', 'out', 'loud', 'and', 'all', 'the', 'girl', 'ğŸ‘­', 'ğŸ‘­', 'were', 'completely', 'vibing', 'to', 'it', 'ğŸ’', 'ğŸ’', 'and', 'they', 'were', 'like', 'slayyyyy', 'ğŸ”ª', 'ğŸ”ª', 'sisterrr', 'â˜ ', 'ï¸', 'â˜ ', 'ï¸and', 'i', 'wa', 'gon', 'na', 'say', 'something', 'back', 'when', 'a', 'boy', 'ğŸ¤®', 'ğŸ¤®', 'approached', 'me', 'and', 'said', 'ğŸ—£', 'ï¸', 'ğŸ—£', 'ï¸', 'uh', 'billie', 'ğŸ¤®', 'ğŸ¤®', 'eilish', 'is', 'so', 'cringe', 'why', 'dont', 'you', 'ğŸ¤¢', 'ğŸ¤¢', 'listen', 'to', 'kanye', 'ğŸ§', 'ğŸ§', 'and', 'i', 'wa', 'shook', 'ğŸ˜²', 'ğŸ˜²', 'ğŸ˜³', 'ğŸ˜³', 'and', 'completely', 'flipped', 'the', 'f', 'out', 'ğŸ¤¬', 'ğŸ¤¬', 'i', 'said', 'you', 'dumb', 'ğŸ¤¬', 'ğŸ¤¬', 'ignorant', 'ğŸ‘¨ğŸ»', '\\u200d', 'ğŸ¦°', 'ğŸ‘¨ğŸ»', '\\u200d', 'ğŸ¦°', 'male', 'billie', 'literally', 'ğŸ™ğŸ»', 'ğŸ™ğŸ»', 'ğŸ™ğŸ»', 'saved', 'my', 'life', 'ğŸ™', 'ğŸ™', 'i', 'wa', 'cutting', 'myself', 'ğŸ˜«', 'ğŸ˜«', 'for', 'my', 'depression', 'since', 'daddy', 'ğŸ§”ğŸ»', 'ğŸ§”ğŸ»', 'didnt', 'get', 'me', 'ticket', 'to', 'coachella', 'ğŸ‘´', 'ğŸ‘´', 'and', 'a', 'pentagram', 'â›§', 'formed', 'on', 'the', 'ground', 'and', 'billie', 'rose', 'ğŸ‘†ğŸ»', 'ğŸ‘†ğŸ»', 'ğŸ‘†ğŸ»', 'ğŸ‘†ğŸ»', 'up', 'from', 'it', 'ğŸ§–\\u200dâ™€ï¸', 'ğŸ§–\\u200dâ™€ï¸', 'and', 'she', 'said', 'put', 'your', 'faith', 'in', 'allah', 'for', 'he', 'is', 'the', 'most', 'merciful', 'ğŸ’£', 'ğŸ’£', 'and', 'then', 'she', 'left', 'and', 'i', 'wa', 'so', 'inspired', 'that', 'i', 'read', 'the', 'korean', 'promised', 'to', 'slay', 'ğŸ”ª', 'ğŸ”ª', 'ğŸ”ª', 'every', 'infidel', 'in', 'my', 'path', 'ğŸ§•', 'ğŸ§•', 'until', 'shariah', 'law', 'wa', 'implemented', 'world', 'wide', 'he', 'then', 'wa', 'like', 'i', 'wont', 'allow', 'a', 'mujahid', 'to', 'spread', 'the', 'gentle', 'message', 'of', 'mohamabamba', 'and', 'then', 'summoned', 'a', 'djinn', 'ğŸ§\\u200dâ™‚ï¸', 'ğŸ§\\u200dâ™‚ï¸', 'in', 'the', 'shape', 'of', 'jahsehs', 'foreskin', 'and', 'he', 'said', 'he', 'wa', 'the', 'servant', 'of', 'shaytan', 'ğŸ‘¹', 'ğŸ‘¹', 'well', 'i', 'wasnt', 'going', 'to', 'let', 'blasphemy', 'ğŸ˜¡', 'ğŸ˜¡', 'go', 'unpunished', 'and', 'chanted', 'oh', 'allah', 'the', 'most', 'kind', 'and', 'beautiful', 'please', 'banish', 'these', 'heathen', 'back', 'to', 'hell', 'ğŸ”™', 'ğŸ”™', 'and', 'suddenly', 'the', 'heaven', 'opened', 'and', 'we', 'loooked', 'and', 'it', 'wa', 'billie', 'ğŸ¤©', 'ğŸ¤©', 'she', 'said', 'i', 'am', 'allah', 'ğŸ™‡ğŸ»', '\\u200d', 'â™‚', '</s>']\n",
      "[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-0.7427, -0.1527, -0.3703,  ...,  0.7159, -0.0371, -0.0536],\n",
      "        [-1.5682, -0.6376, -0.5195,  ...,  1.8709, -0.5231,  0.1583],\n",
      "        ...,\n",
      "        [-0.1862, -0.0678, -5.3837,  ...,  1.0025, -0.3832, -0.1431],\n",
      "        [-0.2198, -0.1540, -1.4406,  ...,  0.3978, -0.2948,  0.0774],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'hello', 'i', 'am', 'xxbedwarsplayzyt', 'i', 'have', 'been', 'playing', 'on', 'your', 'server', 'for', 'over', '2', 'year', 'now', 'yesterday', 'i', 'got', 'banned', 'ğŸ”¨', 'for', 'using', 'a', 'hacked', 'client', 'ğŸ’»', 'this', 'wa', 'a', 'complete', 'accident', 'i', 'swear', 'ğŸ˜­', 'i', 'wa', 'playing', 'on', 'my', 'survival', 'world', 'and', 'i', 'needed', 'diamond', 'so', 'i', 'installed', 'a', 'hack', 'client', 'and', 'turn', 'ed', 'on', 'auto', 'mine', 'and', 'other', 'hack', 'so', 'i', 'get', 'diamond', 'ğŸ˜‚', 'i', 'got', 'bored', 'so', 'i', 'went', 'onto', 'your', 'server', 'ğŸ‘Œ', 'i', 'didnt', 'know', 'that', 'i', 'had', 'hack', 'ğŸ’»', 'on', 'because', 'i', 'just', 'opened', 'up', 'my', 'inventory', 'to', 'go', 'into', 'game', '7', 'second', 'later', 'i', 'wa', 'banned', 'for', 'using', 'hack', 'ğŸ˜³', 'ğŸ˜±', 'i', 'am', 'extremely', 'sorry', 'for', 'this', 'i', 'hope', 'you', 'understand', 'please', 'unban', 'ğŸ™', 'me', 'im', 'literally', 'shaking', 'and', 'cry', 'right', 'now', 'ğŸ˜­', 'ğŸ˜­', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-0.2904,  0.0321, -0.2804,  ...,  0.3336,  0.0031, -0.0874],\n",
      "        [-3.0416, -1.1281, -0.6507,  ...,  1.9127,  1.8111,  0.4478],\n",
      "        ...,\n",
      "        [-0.7745, -0.5737, -0.8827,  ...,  3.1166,  0.5821,  1.3847],\n",
      "        [-0.7745, -0.5737, -0.8827,  ...,  3.1166,  0.5821,  1.3847],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n"
     ]
    }
   ],
   "source": [
    "sample = train.sample(5)\n",
    "for index,row in sample.iterrows():\n",
    "    print(row[\"words\"])\n",
    "    print(row[\"labels\"])\n",
    "    print(trainEmb[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building supervised models to predict next emoji\n",
    "\n",
    "+ RNN based architecture where we look at the hidden layer for every word\n",
    "  + Using hidden layer, predict if is an emoji and what emoji it is\n",
    "+ Asked TA from NLP class, they said this is similar to a language modelling problem where we only predict the set of emoji vocabulary\n",
    "  + Could also view as sequence labelling where tag is next emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
